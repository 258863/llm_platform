# LLM Platform

åŸºäºV100 GPUé›†ç¾¤çš„å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å¹³å°ï¼Œæ”¯æŒå¤šç§æ¨¡å‹éƒ¨ç½²ã€çŸ¥è¯†åº“é›†æˆå’Œå¯è§†åŒ–ç•Œé¢ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸš€ å¤šæ¨¡å‹æ”¯æŒ
  - æ”¯æŒDeepSeek-R1ã€ChatGLM3ç­‰ä¸»æµå¤§æ¨¡å‹
  - æ”¯æŒæ¨¡å‹åŠ¨æ€åˆ‡æ¢
  - æ”¯æŒæ¨¡å‹å¹¶è¡Œæ¨ç†

- ğŸ“š çŸ¥è¯†åº“é›†æˆ
  - æ”¯æŒæ–‡æ¡£çŸ¥è¯†åº“æ„å»º
  - æ”¯æŒå‘é‡æ•°æ®åº“å­˜å‚¨
  - æ”¯æŒRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ

- ğŸ–¥ï¸ å¯è§†åŒ–ç•Œé¢
  - Web UIç•Œé¢
  - GridIOç®€æ˜“ç•Œé¢
  - ç»ˆç«¯äº¤äº’æ¨¡å¼

- ğŸ”Œ APIæœåŠ¡
  - FastAPI RESTfulæ¥å£
  - WebSocketå®æ—¶å¯¹è¯
  - APIæ–‡æ¡£è‡ªåŠ¨ç”Ÿæˆ

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- GPU: 4x V100 (16GB/32GB)
  - æœ€ä½é…ç½®: 2x V100 16GB
  - æ¨èé…ç½®: 4x V100 32GB
  - æ”¯æŒGPUå¹¶è¡Œæ¨ç†
- RAM: 64GB+
  - æœ€ä½é…ç½®: 64GB
  - æ¨èé…ç½®: 128GB
  - ç”¨äºæ¨¡å‹åŠ è½½å’Œæ¨ç†
- Storage: 100GB+
  - æœ€ä½é…ç½®: 100GB SSD
  - æ¨èé…ç½®: 500GB NVMe SSD
  - ç”¨äºå­˜å‚¨æ¨¡å‹æ–‡ä»¶ã€çŸ¥è¯†åº“å’Œæ—¥å¿—
- Network: åƒå…†ç½‘ç»œ
  - ç”¨äºæ¨¡å‹ä¸‹è½½å’ŒAPIè®¿é—®

### è½¯ä»¶è¦æ±‚
- OS: Linux/Windows
  - Linux: Ubuntu 20.04 LTSæˆ–æ›´é«˜ç‰ˆæœ¬
  - Windows: Windows 10/11 ä¸“ä¸šç‰ˆ
- CUDA: 11.8+
- Python: 3.8+
- Docker: 20.10+ (å¯é€‰)

## æ¨¡å‹è§„æ ¼

### DeepSeek-R1
- æ¨¡å‹å¤§å°: 7B/13B/33Bå‚æ•°
- æ˜¾å­˜å ç”¨:
  - 7B: çº¦14GBæ˜¾å­˜
  - 13B: çº¦26GBæ˜¾å­˜
  - 33B: çº¦66GBæ˜¾å­˜
- é‡åŒ–ç‰ˆæœ¬:
  - 7B-Q4_K_M: çº¦4GBæ˜¾å­˜
  - 13B-Q4_K_M: çº¦8GBæ˜¾å­˜
  - 33B-Q4_K_M: çº¦20GBæ˜¾å­˜
- ä¸‹è½½å¤§å°:
  - 7B: çº¦14GB
  - 13B: çº¦26GB
  - 33B: çº¦66GB

### ChatGLM3
- æ¨¡å‹å¤§å°: 6B/32Bå‚æ•°
- æ˜¾å­˜å ç”¨:
  - 6B: çº¦12GBæ˜¾å­˜
  - 32B: çº¦64GBæ˜¾å­˜
- é‡åŒ–ç‰ˆæœ¬:
  - 6B-Q4_K_M: çº¦4GBæ˜¾å­˜
  - 32B-Q4_K_M: çº¦16GBæ˜¾å­˜
- ä¸‹è½½å¤§å°:
  - 6B: çº¦12GB
  - 32B: çº¦64GB

### Qwen
- æ¨¡å‹å¤§å°: 7B/14B/72Bå‚æ•°
- æ˜¾å­˜å ç”¨:
  - 7B: çº¦14GBæ˜¾å­˜
  - 14B: çº¦28GBæ˜¾å­˜
  - 72B: çº¦144GBæ˜¾å­˜
- é‡åŒ–ç‰ˆæœ¬:
  - 7B-Q4_K_M: çº¦4GBæ˜¾å­˜
  - 14B-Q4_K_M: çº¦8GBæ˜¾å­˜
  - 72B-Q4_K_M: çº¦32GBæ˜¾å­˜
- ä¸‹è½½å¤§å°:
  - 7B: çº¦14GB
  - 14B: çº¦28GB
  - 72B: çº¦144GB

## å¿«é€Ÿå¼€å§‹

### WSL Ubuntuç¯å¢ƒé…ç½®

1. å®‰è£…WSLå’ŒUbuntu
```bash
# Windows PowerShell (ç®¡ç†å‘˜)
wsl --install -d Ubuntu

# è®¾ç½®WSLç‰ˆæœ¬ä¸ºWSL2
wsl --set-default-version 2
```

2. é…ç½®WSLç¯å¢ƒ
```bash
# æ›´æ–°ç³»ç»Ÿ
sudo apt update && sudo apt upgrade -y

# å®‰è£…å¿…è¦çš„ç³»ç»ŸåŒ…
sudo apt install -y build-essential git curl wget python3-pip

# å®‰è£…CUDAæ”¯æŒ
# è®¿é—® https://developer.nvidia.com/cuda-downloads
# é€‰æ‹© Linux -> x86_64 -> Ubuntu -> 22.04 -> deb (network)
# æŒ‰ç…§æç¤ºå®‰è£…CUDA
```

3. é¡¹ç›®è®¾ç½®
```bash
# åœ¨WSLä¸­åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p ~/projects
cd ~/projects

# å…‹éš†é¡¹ç›®åˆ°WSLæœ¬åœ°ç›®å½•
git clone https://github.com/yourusername/llm_platform.git

# æˆ–è€…ä»Windowså¤åˆ¶é¡¹ç›®æ–‡ä»¶åˆ°WSL
# åœ¨Windows PowerShellä¸­æ‰§è¡Œï¼š
wsl --distribution Ubuntu --user yourusername cp -r /mnt/c/Users/YourUsername/Projects/llm_platform/* ~/projects/llm_platform/
```

4. é…ç½®Pythonç¯å¢ƒ
```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd ~/projects/llm_platform

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

5. é…ç½®Ollama
```bash
# å®‰è£…Ollama
curl https://ollama.ai/install.sh | sh

# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p ~/projects/llm_platform/models/ollama

# è®¾ç½®OLLAMA_MODELSç¯å¢ƒå˜é‡
echo 'export OLLAMA_MODELS=~/projects/llm_platform/models/ollama' >> ~/.bashrc
source ~/.bashrc

# ä¸‹è½½æ¨¡å‹
ollama pull deepseek-r1
ollama pull chatglm3
ollama pull qwen
```

6. å¯åŠ¨æœåŠ¡
```bash
# ç¡®ä¿åœ¨é¡¹ç›®ç›®å½•ä¸‹
cd ~/projects/llm_platform

# å¯åŠ¨APIæœåŠ¡
python api/main.py

# æ–°å¼€ä¸€ä¸ªç»ˆç«¯ï¼Œå¯åŠ¨Web UI
python ui/main.py
```

7. è®¿é—®æœåŠ¡
- APIæ–‡æ¡£: http://localhost:8000/docs
- Web UI: http://localhost:8080
- GridIO: http://localhost:8081

### æ³¨æ„äº‹é¡¹
1. WSL2æ€§èƒ½ä¼˜åŒ–
```bash
# åœ¨Windowsä¸­åˆ›å»ºæˆ–ç¼–è¾‘ %UserProfile%\.wslconfig æ–‡ä»¶
[wsl2]
memory=16GB
processors=8
localhostForwarding=true
```

2. æ–‡ä»¶åŒæ­¥
- å»ºè®®åœ¨WSLä¸­ç›´æ¥å¼€å‘ï¼Œè€Œä¸æ˜¯åœ¨Windowsä¸­å¼€å‘
- å¦‚æœéœ€è¦åŒæ­¥ä»£ç ï¼Œå¯ä»¥ä½¿ç”¨gitè¿›è¡Œç‰ˆæœ¬æ§åˆ¶
- æˆ–è€…ä½¿ç”¨VSCodeçš„Remote-WSLæ‰©å±•ï¼Œç›´æ¥åœ¨WSLä¸­ç¼–è¾‘ä»£ç 

3. ç«¯å£è½¬å‘
- WSL2é»˜è®¤æ”¯æŒç«¯å£è½¬å‘ï¼Œæ— éœ€é¢å¤–é…ç½®
- å¦‚æœæ— æ³•è®¿é—®ï¼Œæ£€æŸ¥Windowsé˜²ç«å¢™è®¾ç½®

4. GPUæ”¯æŒ
- ç¡®ä¿Windowså·²å®‰è£…NVIDIAé©±åŠ¨
- åœ¨WSLä¸­å®‰è£…CUDAå·¥å…·åŒ…
- éªŒè¯GPUå¯ç”¨æ€§ï¼š
```bash
nvidia-smi
```

5. å¼€å‘å»ºè®®
- ä½¿ç”¨VSCode + Remote-WSLæ‰©å±•è¿›è¡Œå¼€å‘
- åœ¨WSLä¸­ç›´æ¥è¿è¡Œå’Œæµ‹è¯•ä»£ç 
- é¿å…åœ¨Windowså’ŒWSLä¹‹é—´é¢‘ç¹åˆ‡æ¢
- ä½¿ç”¨gitè¿›è¡Œä»£ç ç‰ˆæœ¬æ§åˆ¶

### å…¶ä»–ç¯å¢ƒé…ç½®

1. å®‰è£…ä¾èµ–
```bash
pip install -r requirements.txt
```

2. é…ç½®ç¯å¢ƒ
```bash
cp config/config.example.yaml config/config.yaml
# ç¼–è¾‘ config.yaml é…ç½®æ–‡ä»¶
```

3. å¯åŠ¨æœåŠ¡
```bash
# å¯åŠ¨APIæœåŠ¡
python api/main.py

# å¯åŠ¨Web UI
python ui/main.py

# å¯åŠ¨GridIOç•Œé¢
python ui/gridio_app.py
```

4. è®¿é—®æœåŠ¡
- APIæ–‡æ¡£: http://localhost:8000/docs
- Web UI: http://localhost:8080
- GridIO: http://localhost:8081

## è¯¦ç»†éƒ¨ç½²æŒ‡å—

### 1. æ¨¡å‹éƒ¨ç½²

#### 1.1 ä¸‹è½½æ¨¡å‹
```bash
# æ–¹å¼1ï¼šä½¿ç”¨Ollamaï¼ˆæ¨èï¼‰
# 1. åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models/ollama

# 2. å®‰è£…Ollamaå¹¶æŒ‡å®šæ¨¡å‹ç›®å½•
# Linux
curl https://ollama.ai/install.sh | sh
# è®¾ç½®OLLAMA_MODELSç¯å¢ƒå˜é‡
export OLLAMA_MODELS=models/ollama
# æ·»åŠ åˆ°.bashrcæˆ–.zshrc
echo 'export OLLAMA_MODELS=models/ollama' >> ~/.bashrc

# Windows
# ä¸‹è½½å®‰è£…åŒ…ï¼šhttps://ollama.ai/download
# è®¾ç½®ç¯å¢ƒå˜é‡ OLLAMA_MODELS=C:\path\to\models\ollama

# 3. ä¸‹è½½æ¨¡å‹åˆ°æŒ‡å®šç›®å½•
ollama pull deepseek-r1
ollama pull chatglm3
ollama pull qwen

# 4. éªŒè¯æ¨¡å‹ä½ç½®
ls models/ollama
# åº”è¯¥èƒ½çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„ç›®å½•ç»“æ„ï¼š
# models/ollama/
# â”œâ”€â”€ deepseek-r1/
# â”œâ”€â”€ chatglm3/
# â””â”€â”€ qwen/

# æ–¹å¼2ï¼šä½¿ç”¨huggingface-cli
# åˆ›å»ºæ¨¡å‹ç›®å½•
mkdir -p models/deepseek-r1
mkdir -p models/chatglm3
mkdir -p models/qwen

# ä¸‹è½½DeepSeek-R1æ¨¡å‹
huggingface-cli download deepseek-ai/deepseek-r1 --local-dir models/deepseek-r1

# ä¸‹è½½ChatGLM3æ¨¡å‹
git clone https://huggingface.co/THUDM/chatglm3-6b models/chatglm3

# ä¸‹è½½Qwenæ¨¡å‹
git clone https://huggingface.co/Qwen/Qwen-7B models/qwen
```

#### 1.2 æ¨¡å‹é…ç½®
åœ¨ `config/config.yaml` ä¸­é…ç½®æ¨¡å‹è·¯å¾„ï¼š
```yaml
models:
  deepseek-r1:
    path: "models/ollama/deepseek-r1"  # æŒ‡å®šOllamaæ¨¡å‹è·¯å¾„
    type: "deepseek"
    max_length: 2048
    temperature: 0.7
  chatglm3:
    path: "models/ollama/chatglm3"  # æŒ‡å®šOllamaæ¨¡å‹è·¯å¾„
    type: "chatglm"
    max_length: 2048
    temperature: 0.7
  qwen:
    path: "models/ollama/qwen"  # æŒ‡å®šOllamaæ¨¡å‹è·¯å¾„
    type: "qwen"
    max_length: 2048
    temperature: 0.7
```

### 2. æ•°æ®åº“é…ç½®

#### 2.1 å‘é‡æ•°æ®åº“
```bash
# åˆ›å»ºå‘é‡æ•°æ®åº“ç›®å½•
mkdir -p data/vector_db

# é…ç½®å‘é‡æ•°æ®åº“è·¯å¾„
# åœ¨ config/config.yaml ä¸­è®¾ç½®ï¼š
vector_db:
  path: "data/vector_db"
  collection: "documents"
```

#### 2.2 æ–‡æ¡£å­˜å‚¨
```bash
# åˆ›å»ºæ–‡æ¡£å­˜å‚¨ç›®å½•
mkdir -p data/documents

# æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ï¼š
# - PDF: data/documents/pdf/
# - Word: data/documents/word/
# - Markdown: data/documents/markdown/
# - HTML: data/documents/html/
```

### 3. æ—¥å¿—é…ç½®

```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# åœ¨ config/config.yaml ä¸­é…ç½®æ—¥å¿—ï¼š
logging:
  level: "INFO"
  file: "logs/app.log"
  max_size: 100  # MB
  backup_count: 10
```

### 4. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š
```bash
# å¤åˆ¶ç¯å¢ƒå˜é‡æ¨¡æ¿
cp .env.example .env

# ç¼–è¾‘ .env æ–‡ä»¶
# è®¾ç½®å¿…è¦çš„ç¯å¢ƒå˜é‡
MODEL_PATH=models
VECTOR_DB_PATH=data/vector_db
DOC_PATH=data/documents
LOG_PATH=logs
```

## ç³»ç»Ÿæ¶æ„

```mermaid
graph TB
    %% å®šä¹‰æ ·å¼
    classDef client fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#0d47a1,rx:10,ry:10;
    classDef server fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,rx:10,ry:10;
    classDef model fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#1b5e20,rx:10,ry:10;
    classDef kb fill:#fff3e0,stroke:#f57c00,stroke-width:2px,color:#e65100,rx:10,ry:10;
    classDef storage fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#880e4f,rx:10,ry:10;

    %% å®šä¹‰è¿æ¥çº¿æ ·å¼
    linkStyle default stroke:#616161,stroke-width:2px,stroke-dasharray: 5 5;

    subgraph Client["å®¢æˆ·ç«¯å±‚"]
        direction TB
        A[Web UI]:::client --> |HTTP/WebSocket| API
        B[GridIO UI]:::client --> |HTTP/WebSocket| API
        C[CLI]:::client --> |HTTP| API
    end

    subgraph Server["æœåŠ¡å™¨å±‚"]
        direction TB
        API[API Service]:::server --> |è¯·æ±‚å¤„ç†| Core
        Core[Core Service]:::server --> |æ¨¡å‹è°ƒç”¨| Models
        Core --> |çŸ¥è¯†æ£€ç´¢| KB[Knowledge Base]
        
        subgraph Models["æ¨¡å‹æœåŠ¡"]
            direction LR
            M1[DeepSeek-R1]:::model
            M2[ChatGLM3]:::model
            M3[Qwen]:::model
        end
        
        subgraph KB["çŸ¥è¯†åº“"]
            direction LR
            V1[å‘é‡æ•°æ®åº“]:::kb
            V2[æ–‡æ¡£å­˜å‚¨]:::kb
            V1 --> |Embedding| V2
        end
    end

    subgraph Storage["å­˜å‚¨å±‚"]
        direction LR
        D1[(æ¨¡å‹æ–‡ä»¶)]:::storage
        D2[(é…ç½®æ–‡ä»¶)]:::storage
        D3[(æ—¥å¿—æ–‡ä»¶)]:::storage
    end

    Models --> D1
    Core --> D2
    API --> D3

    %% æ·»åŠ ç‰¹æ®Šè¿æ¥çº¿æ ·å¼
    linkStyle 0,1,2,3,4,5,6,7,8,9,10,11 stroke:#616161,stroke-width:2px;
```

## é¡¹ç›®ç»“æ„

```
llm_platform/
â”œâ”€â”€ api/            # FastAPIæ¥å£æœåŠ¡
â”œâ”€â”€ core/           # æ ¸å¿ƒåŠŸèƒ½å®ç°
â”œâ”€â”€ ui/             # å¯è§†åŒ–ç•Œé¢
â”œâ”€â”€ utils/          # å·¥å…·å‡½æ•°
â”œâ”€â”€ docs/           # æ–‡æ¡£
â”œâ”€â”€ models/         # æ¨¡å‹æ–‡ä»¶
â”œâ”€â”€ data/           # æ•°æ®æ–‡ä»¶
â””â”€â”€ config/         # é…ç½®æ–‡ä»¶
```

## ä½¿ç”¨è¯´æ˜

### 1. æ¨¡å‹éƒ¨ç½²

æ”¯æŒä»¥ä¸‹æ¨¡å‹ï¼š
- DeepSeek-R1 (7B/13B/33B)
- ChatGLM3 (6B/32B)
- Qwen (7B/14B/72B)

### 2. çŸ¥è¯†åº“åŠŸèƒ½

æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼š
- PDF
- Word
- Markdown
- HTML

### 3. APIè°ƒç”¨

```python
import requests

# å¯¹è¯æ¥å£
response = requests.post(
    "http://localhost:8000/api/chat",
    json={
        "message": "ä½ å¥½",
        "model": "deepseek-r1",
        "use_knowledge": True
    }
)
```

### 4. ç•Œé¢ä½¿ç”¨

- Web UI: æä¾›å®Œæ•´çš„å¯¹è¯å’ŒçŸ¥è¯†åº“ç®¡ç†ç•Œé¢
- GridIO: æä¾›ç®€å•çš„å¯¹è¯ç•Œé¢
- ç»ˆç«¯: æ”¯æŒå‘½ä»¤è¡Œäº¤äº’

## å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒæ›´å¤šæ¨¡å‹
- [ ] ä¼˜åŒ–æ¨ç†æ€§èƒ½
- [ ] æ·»åŠ æ›´å¤šçŸ¥è¯†åº“æ ¼å¼
- [ ] æ”¹è¿›ç”¨æˆ·ç•Œé¢
- [ ] æ·»åŠ ç›‘æ§åŠŸèƒ½

## è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## è®¸å¯è¯

MIT License 

```mermaid
graph TB
    %% å®šä¹‰æ ·å¼
    classDef start fill:#e3f2fd,stroke:#1976d2,stroke-width:2px,color:#0d47a1,rx:10,ry:10;
    classDef process fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#4a148c,rx:10,ry:10;
    classDef decision fill:#e8f5e9,stroke:#388e3c,stroke-width:2px,color:#1b5e20,rx:10,ry:10;
    classDef end fill:#fce4ec,stroke:#c2185b,stroke-width:2px,color:#880e4f,rx:10,ry:10;

    %% å¼€å§‹
    Start[é¡¹ç›®å¯åŠ¨]:::start
    
    %% ç¯å¢ƒé…ç½®
    Env[ç¯å¢ƒé…ç½®]:::process
    Start --> Env
    
    %% æ¨¡å‹éƒ¨ç½²
    Model[æ¨¡å‹éƒ¨ç½²]:::process
    Env --> Model
    
    %% çŸ¥è¯†åº“æ„å»º
    KB[çŸ¥è¯†åº“æ„å»º]:::process
    Model --> KB
    
    %% ç•Œé¢å¼€å‘
    UI[ç•Œé¢å¼€å‘]:::process
    KB --> UI
    
    %% APIå¼€å‘
    API[APIå¼€å‘]:::process
    UI --> API
    
    %% æµ‹è¯•
    Test{æµ‹è¯•}:::decision
    API --> Test
    
    %% æµ‹è¯•ç»“æœåˆ¤æ–­
    Test -->|é€šè¿‡| Deploy[éƒ¨ç½²]:::end
    Test -->|ä¸é€šè¿‡| Fix[ä¿®å¤]:::process
    Fix --> API
    
    %% è¿æ¥çº¿æ ·å¼
    linkStyle default stroke:#616161,stroke-width:2px;
```

## é¡¹ç›®ç»“æ„ 